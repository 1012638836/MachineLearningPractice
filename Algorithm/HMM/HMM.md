# Hidden Markov Model

## Overview

The HMM is a generative probabilistic model, in which a sequence of observable X variables is generated by a sequence of internal hidden states Z. The hidden states are not be observed directly. The transitions between hidden states are assumed to have the form of a (first-order) Markov chain. They can be specified by the start probability vector π and a transition probability matrix A. The emission probability of an observable can be any distribution with parameters θ conditioned on the current hidden state. The HMM is completely determined by π, A and θ.

There are three fundamental problems for HMMs:

* Given the model parameters and observed data, estimate the optimal sequence of hidden states.
* Given the model parameters and observed data, calculate the likelihood of the data.
* Given just the observed data, estimate the model parameters.

The first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively.
The last one can be solved by an iterative *Expectation-Maximization (EM) algorithm*, known as the Baum-Welch algorithm.

## [EM Algorithm](../EM/EM.md)

### Package

#### HMMLearn

* [Github hmmlearn/hmmlearn](https://github.com/hmmlearn/hmmlearn)
* [Document](https://hmmlearn.readthedocs.io/en/stable/)

#### HanLP

* [Python API Github](https://github.com/hankcs/pyhanlp)
* [Official Main Page](http://hanlp.com/)
